{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T01:45:42.132649Z",
     "start_time": "2021-03-12T01:45:42.126652Z"
    }
   },
   "outputs": [],
   "source": [
    "############### import packages\n",
    "import os, requests, sys, re, pandas as pd, time, urllib.request, csv, gc\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '8-K'\n",
    "period_start = 2014 # included\n",
    "period_end = 2020 # included\n",
    "raw_filing_dir = 'G:\\\\8-K\\\\' ########## directory where you want to save the downloaded EDGAR filings\n",
    "master_index_dir = 'F:\\\\github\\\\python-edgar-master\\\\edgar-idx' ######### directory where the edgar master index are saved\n",
    "output_csv_dir = r'..\\filings\\id_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) +'.csv' ######### directory of the output id_data.csv\n",
    "time_waiting = 1/20 ########## sleeping time between the scraping of each filing in order to avoid being blocked by EDGAR\n",
    "begin_from = 357203\n",
    "memory_limit = 95\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "# if os.getcwd() != r'F:\\github\\narrative_conservatism\\code':\n",
    "#     os.chdir(r'F:\\github\\narrative_conservatism\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T01:45:45.238875Z",
     "start_time": "2021-03-12T01:45:45.184906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Access all fillings through SEC master index #################################\n",
    "####### indexes downloaded using python-edgar: https://github.com/edouardswiac/python-edgar #######\n",
    "#### open terminal, and run the following lines:\n",
    "#### cd F:\\github\\python-edgar-master (switch dir to where the run.py script is located)\n",
    "#### python run.py -y 1993 -d edgar_idx (downloading all quarterly master index from 1993 into folder edgar_idx)\n",
    "\n",
    "#### cd F:\\github\\python-edgar-master\\edgar-idx (switch dir to where the downloaded indexes are located)\n",
    "#### cat *.tsv > master.tsv (stitch all quarterly indexes into one master index)\n",
    "#### du -h master.tsv (inspect how large the master index file is)\n",
    "\n",
    "index_edgar = list()\n",
    "doc_url = list()\n",
    "\n",
    "# create an index of downloaded local quarterly master indexes\n",
    "for subdir, dirs, files in os.walk(master_index_dir):\n",
    "    for file in files:\n",
    "        file_year = int(file.split('-')[0])\n",
    "        if file_year >= period_start and file_year <= period_end:\n",
    "            index_edgar.append(os.path.join(subdir, file))\n",
    "\n",
    "# read each index file, select rows with matched file type, and store matched doc_links\n",
    "for filenameTSV in index_edgar:\n",
    "    tsv_read = pd.read_csv(filenameTSV, sep='|', header=None, encoding = \"utf-8\")\n",
    "    tsv_read.columns = ['1', '2', '3', '4', '5', '6']\n",
    "    \n",
    "    # select the rows with filetype equal to predefined type\n",
    "    tsv_type = tsv_read.loc[tsv_read['3'] == obj_type]\n",
    "    doc_link = tsv_type['6'].values.tolist()\n",
    "    doc_link = ['https://www.sec.gov/Archives/' + w for w in doc_link]\n",
    "    for doc in doc_link:\n",
    "        doc_url.append(doc)\n",
    "\n",
    "del index_edgar\n",
    "len(doc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T01:45:46.517144Z",
     "start_time": "2021-03-12T01:45:46.450185Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-b5f4f9bce628>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[0mweb_url\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc_url\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbegin_from\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mpsutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvirtual_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpercent\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mmemory_limit\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m         \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'doc_url' is not defined"
     ]
    }
   ],
   "source": [
    "############### Extract file identification info from doc_url\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "\n",
    "if os.path.exists(output_csv_dir) == False:\n",
    "    with open(output_csv_dir, mode='w') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['accnum','cik','name','fd', 'rp','item8k','sic','fye','state','bazip','irs','film','pdc','accepted','nexhibit','ngraph','web_url'])\n",
    "\n",
    "## define lists\n",
    "accnum = []\n",
    "cik = []\n",
    "name = []\n",
    "fd = []\n",
    "rp = []\n",
    "item8k = []\n",
    "sic = []\n",
    "fye = []\n",
    "state = []\n",
    "bazip = []\n",
    "irs = []\n",
    "film = []\n",
    "pdc = []\n",
    "accepted = []\n",
    "nexhibit = []\n",
    "ngraph = []\n",
    "web_url = []\n",
    "\n",
    "for doc in tqdm(doc_url[begin_from:]):\n",
    "    if psutil.virtual_memory().percent > memory_limit:\n",
    "        break\n",
    "        \n",
    "    time.sleep(time_waiting) # SEC does not allow to exceed 10 requests/sec\n",
    "    doc_resp = requests.get(doc, headers=headers)\n",
    "    if doc_resp.status_code == 429:\n",
    "        time.sleep(10*60+5) # if exceeds cool off for 10 mins\n",
    "        doc_resp = requests.get(doc, headers=headers)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    soup = BeautifulSoup(doc_resp.text, 'html.parser')\n",
    "\n",
    "    # Save the SEC accession number (accnum)\n",
    "    try:\n",
    "        accnum_i = soup.find('div', id='formHeader').find('div', id='secNum').get_text().split()[3]\n",
    "        accnum.append(accnum_i)\n",
    "    except:\n",
    "        accnum.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "    # Save the Filing Date (0), Accepted Date (Date as of Change) (1), Public Document Count (2) and Reporting Period (3)\n",
    "    try:\n",
    "        dates = soup.find('div', class_='formContent').find_all('div', class_='info')\n",
    "        # Filing Date\n",
    "        fd.append(dates[0].get_text())\n",
    "    except:\n",
    "        fd.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # Accepted Date (Date as of Change)\n",
    "    try:\n",
    "        accepted.append(dates[1].get_text())\n",
    "    except:\n",
    "        accepted.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # Public Document Count\n",
    "    try:\n",
    "        pdc.append(dates[2].get_text())\n",
    "    except:\n",
    "        pdc.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # Reporting Period\n",
    "    try:\n",
    "        rp.append(dates[3].get_text())\n",
    "    except:\n",
    "        rp.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "    # For 8K files, Save item info\n",
    "    try:\n",
    "        if obj_type == '8-K':\n",
    "            clist = re.findall(r'\\d.\\d\\d', dates[4].get_text())\n",
    "            if clist != []:\n",
    "                item8k.append(', '.join(clist))\n",
    "            else:\n",
    "                clist = re.findall(r'\\d+', c = dates[4].get_text())\n",
    "                item8k.append(', '.join(clist))\n",
    "        else :\n",
    "            item8k.append(float('NaN'))\n",
    "    except:\n",
    "        item8k.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "    # Save the Company name and CIK\n",
    "    try:\n",
    "        comname = soup.find('div', class_='companyInfo').find('span', class_='companyName')\n",
    "        # Company Name\n",
    "        name.append(comname.get_text().split(\"\\n\")[0].replace(' (Filer)', ''))\n",
    "    except:\n",
    "        name.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # CIK\n",
    "    try:\n",
    "        cik.append(comname.get_text().split(\"\\n\")[1].replace('CIK: ', '').replace(' (see all company filings)', ''))\n",
    "    except:\n",
    "        cik.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "    # Save Business Address ZIP \n",
    "    try:\n",
    "        div_tag = soup.find_all('div', class_='mailer')[1].find_all('span', class_='mailerAddress')[1]\n",
    "        ba = div_tag.get_text()\n",
    "        alist = re.findall(r'\\d\\d\\d\\d\\d', ba)\n",
    "        if alist == []:\n",
    "            div_tag = soup.find_all('div', class_='mailer')[1].find_all('span', class_='mailerAddress')[2]\n",
    "            ba = div_tag.get_text()\n",
    "            alist = re.findall(r'\\d\\d\\d\\d\\d', ba)\n",
    "        bazip.append(', '.join(alist))\n",
    "    except:\n",
    "        bazip.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # Save SIC, Fiscal Year End, State of Incorporation, IRS number and film number\n",
    "    try:\n",
    "        filinginfo = soup.find('div', class_='companyInfo').find('p', class_='identInfo')\n",
    "        # SIC\n",
    "        sic.append(filinginfo.get_text().split(\"SIC:\")[1].split()[0])\n",
    "    except:\n",
    "        sic.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # Save Fiscal Year End\n",
    "    try:\n",
    "        fye.append(filinginfo.get_text().split(\"Fiscal Year End:\")[1].split()[0].replace('Type:', ''))\n",
    "    except:\n",
    "        fye.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # State\n",
    "    try:\n",
    "        state.append(filinginfo.get_text().split(\"State of Incorp.:\")[1].split()[0].replace('Type:', ''))\n",
    "    except:\n",
    "        state.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # IRS number\n",
    "    try:\n",
    "        irs.append(filinginfo.get_text().split(\"IRS No.:\")[1].split()[0].replace('Type:', ''))\n",
    "    except:\n",
    "        irs.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "        # film number\n",
    "    try:\n",
    "        film.append(filinginfo.get_text().split(\"Film No.: \")[1].split()[0].replace('SIC:', ''))\n",
    "    except:\n",
    "        film.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "    # Save the HTML/TXT website urls from doc_url to raw data folder\n",
    "    try:\n",
    "        rows = soup.find('table', class_='tableFile', summary='Document Format Files').find_all('tr')\n",
    "        cell_html = rows[1].find_all('td')\n",
    "        html = cell_html[2].a['href'].replace('ix?doc=/', '')\n",
    "        cell_txt = rows[-1].find_all('td')\n",
    "        txt = cell_txt[2].a['href']\n",
    "\n",
    "        if html.endswith(\"htm\") or html.endswith(\"txt\"):\n",
    "            url = 'https://www.sec.gov' + html\n",
    "        else:\n",
    "            url = 'https://www.sec.gov' + txt\n",
    "        web_url.append(url)\n",
    "\n",
    "    except:\n",
    "        web_url.append(float('NaN'))\n",
    "        pass\n",
    "\n",
    "    # Count number of exhibits and graphics in this filing\n",
    "    try:\n",
    "        ex = 0\n",
    "        graph = 0\n",
    "        for row in rows[2:-1]:\n",
    "            if row.find_all('td')[3].get_text().startswith('EX'):\n",
    "                ex = ex + 1\n",
    "            elif row.find_all('td')[3].get_text().startswith('GRAPHIC'):\n",
    "                graph = graph + 1\n",
    "            else:\n",
    "                pass\n",
    "        ngraph.append(graph)\n",
    "        nexhibit.append(ex)\n",
    "\n",
    "    except:\n",
    "        ngraph.append(float('NaN'))\n",
    "        nexhibit.append(float('NaN'))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save scraped data locally \n",
    "id_data = pd.DataFrame(data={'accnum': accnum, 'cik': cik, 'name': name, 'fd': fd, 'rp': rp, 'item8k': item8k, 'sic': sic,'fye': fye, 'state': state, 'bazip': bazip, \\\n",
    " 'irs': irs, 'film': film, 'pdc': pdc, 'accepted': accepted, 'nexhibit': nexhibit, 'ngraph': ngraph, 'web_url': web_url})\n",
    "id_data_saved = pd.read_csv(output_csv_dir,  dtype = {'cik':str, 'bazip':str, 'sic':str, 'fye':str, 'film':str, 'irs':str, 'web_url':str})\n",
    "id_data = pd.concat([id_data_saved, id_data])\n",
    "id_data.to_csv(output_csv_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the report if necessary (report can be accessed online so actually no need to download)\n",
    "id_data = pd.read_csv(output_csv_dir, dtype = {'cik':str, 'bazip':str, 'sic':str, 'fye':str, 'film':str, 'irs':str, 'web_url':str})\n",
    "try:\n",
    "    if os.path.exists(raw_filing_dir + accnum_i + '.txt') == False:\n",
    "        urllib.request.urlretrieve(url, raw_filing_dir + accnum_i + '.txt')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_data = pd.read_csv(output_csv_dir,  dtype = {'cik':str, 'bazip':str, 'sic':str, 'fye':str, 'film':str, 'irs':str, 'web_url':str})\n",
    "id_data.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### import packages\n",
    "import os, nltk, numpy as np, pandas as pd, time, textstat, re, csv, gc, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '8-K'\n",
    "item_type = '' # to analyze who document, set item_type to 'full'\n",
    "period_start = 1993 # included\n",
    "period_end = 2005 # included\n",
    "# raw_filing_dir = 'H:\\\\data\\\\edgar\\\\processed\\\\' + obj_type + '\\\\' + item_type\n",
    "if (obj_type == '10-K') | (obj_type == '10-Q'):\n",
    "    output_csv_dir = '..\\\\filings\\\\text_data_' + obj_type + '_' + item_type + '_' + str(period_start) + '-' + str(period_end) + '.csv'\n",
    "else:\n",
    "    output_csv_dir = '..\\\\filings\\\\text_data_' + obj_type  + '_' + str(period_start) + '-' + str(period_end) + '.csv'\n",
    "    \n",
    "input_csv_dir = '..\\\\filings\\\\id_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) + '.csv'\n",
    "dictionary_dir = '..\\\\..\\\\dictionary\\\\'\n",
    "time_waiting = 0\n",
    "############### Set working directory to parent directory\n",
    "if os.getcwd() != r'F:\\github\\narrative_conservatism\\code':\n",
    "    os.chdir(r'F:\\github\\narrative_conservatism\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read LM disctionary\n",
    "LM = pd.read_excel(dictionary_dir + 'LM\\\\LoughranMcDonald_MasterDictionary_2018.xlsx', encoding = \"utf-8\")\n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "lm_neg = LM.loc[LM['Negative'] != 0]['Word'].values.tolist()\n",
    "lm_pos = LM.loc[LM['Positive'] != 0]['Word'].values.tolist()\n",
    "lm_uctt = LM.loc[LM['Uncertainty'] != 0]['Word'].values.tolist()\n",
    "lm_lit = LM.loc[LM['Litigious'] != 0]['Word'].values.tolist()\n",
    "lm_cstr = LM.loc[LM['Constraining'] != 0]['Word'].values.tolist()\n",
    "\n",
    "lm_modal1 = LM.loc[LM['Modal'] == 1]['Word'].values.tolist()\n",
    "lm_modal2 = LM.loc[LM['Modal'] == 2]['Word'].values.tolist()\n",
    "lm_modal3 = LM.loc[LM['Modal'] == 3]['Word'].values.tolist()\n",
    "\n",
    "lm_neg = [w.lower() for w in lm_neg]\n",
    "lm_pos = [w.lower() for w in lm_pos]\n",
    "lm_uctt = [w.lower() for w in lm_uctt]\n",
    "lm_lit = [w.lower() for w in lm_lit]\n",
    "lm_cstr = [w.lower() for w in lm_cstr]\n",
    "lm_modal1 = [w.lower() for w in lm_modal1]\n",
    "lm_modal2 = [w.lower() for w in lm_modal2]\n",
    "lm_modal3 = [w.lower() for w in lm_modal3]\n",
    "\n",
    "############## Read and create stop words list\n",
    "lm_stop = []\n",
    "with open(dictionary_dir + 'LM\\\\StopWords_Generic.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        lm_stop.append(line)\n",
    "        \n",
    "lm_stop = [w.lower() for w in lm_stop]\n",
    "\n",
    "############# Create a negation word list\n",
    "gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read GI disctionary\n",
    "GI_cols = ['Entry', 'Source', 'Positiv', 'Negativ']\n",
    "GI = pd.read_excel(dictionary_dir + 'GI\\\\inquirerbasic.xls', encoding = \"utf-8\", usecols = GI_cols)\n",
    "GI = GI[(GI['Entry'].str.endswith('#1') == True) | (GI['Entry'].str.contains('#') == False)]\n",
    "GI['Entry'] = GI['Entry'].str.replace('#1','') \n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "gi_neg = GI.loc[GI['Negativ'].notnull()]['Entry'].values.tolist()\n",
    "gi_pos = GI.loc[GI['Positiv'].notnull()]['Entry'].values.tolist()\n",
    "\n",
    "gi_neg = [w.lower() for w in gi_neg]\n",
    "gi_pos = [w.lower() for w in gi_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Create Henry disctionary (HENRY 2008)\n",
    "hr_neg = ['negative', 'negatives', 'fail', 'fails', 'failing', 'failure', 'weak', 'weakness', 'weaknesses', 'difficult', 'difficulty', 'hurdle', 'hurdles', 'obstacle', 'obstacles', 'slump', 'slumps', 'slumping', 'slumped', 'uncertain', 'uncertainty', 'unsettled', 'unfavorable', 'downturn', 'depressed', 'disappoint', 'disappoints', 'disappointing', 'disappointed', 'disappointment', 'risk', 'risks', 'risky', 'threat', 'threats', 'penalty', 'penalties', 'down', 'decrease', 'decreases', 'decreasing', 'decreased', 'decline', 'declines', 'declining', 'declined', 'fall', 'falls', 'falling', 'fell', 'fallen', 'drop', 'drops', 'dropping', 'dropped', 'deteriorate', 'deteriorates', 'deteriorating', 'deteriorated', 'worsen', 'worsens', 'worsening', 'weaken', 'weakens', 'weakening', 'weakened', 'worse', 'worst', 'low', 'lower', 'lowest', 'less', 'least', 'smaller', 'smallest', 'shrink']\n",
    "hr_pos = ['positive', 'positives', 'success', 'successes', 'successful', 'succeed', 'succeeds', 'succeeding', 'succeeded', 'accomplish', 'accomplishes', 'accomplishing', 'accomplished', 'accomplishment', 'accomplishments', 'strong', 'strength', 'strengths', 'certain', 'certainty', 'definite', 'solid', 'excellent', 'good', 'leading', 'achieve', 'achieves', 'achieved', 'achieving', 'achievement', 'achievements', 'progress', 'progressing', 'deliver', 'delivers', 'delivered', 'delivering', 'leader', 'leading', 'pleased', 'reward', 'rewards', 'rewarding', 'rewarded', 'opportunity', 'opportunities', 'enjoy', 'enjoys', 'enjoying', 'enjoyed', 'encouraged', 'encouraging', 'up', 'increase', 'increases', 'increasing', 'increased', 'rise', 'rises', 'rising', 'rose', 'risen', 'improve', 'improves', 'improving', 'improved', 'improvement', 'improvements', 'strengthen', 'strengthens', 'strengthening', 'strengthened', 'stronger', 'strongest', 'better', 'best', 'more', 'most', 'above', 'record', 'high', 'higher', 'highest', 'greater', 'greatest', 'larger', 'largest', 'grow', 'grows', 'growing', 'grew', 'grown', 'growth', 'expand', 'expands', 'expanding', 'expanded', 'expansion', 'exceed', 'exceeds', 'exceeded', 'exceeding', 'beat', 'beats', 'beating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473538"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#################### FOR ALL PROCESSED FILES LOOP ###################\n",
    "#####################################################################\n",
    "\n",
    "# ############# OPTION 1: Create input txt file index from disk\n",
    "# processed = []\n",
    "# for subdir, dirs, files in os.walk(raw_filing_dir):\n",
    "#     for file in files:\n",
    "#         processed.append(os.path.join(subdir, file))\n",
    "\n",
    "############# OPTION 2: Create input txt file index: from list of true positive files\n",
    "id_data = pd.read_csv(input_csv_dir,  dtype = {'cik':str, 'bazip':str, 'sic':str, 'fye':str, 'film':str, 'irs':str, 'web_url':str})\n",
    "id_data = id_data[(id_data.duplicated('accnum') == False) & (id_data['accnum'].notnull())]\n",
    "web_url = id_data['web_url'].values.tolist()\n",
    "# web_url = random.sample(processed, 10086)\n",
    "len(web_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sec.gov/Archives/edgar/data/60512/0000060512-94-000008.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100240/0000950144-94-000103.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100240/0000950144-94-000177.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100240/0000950144-94-000277.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100880/0000716039-94-000008.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100885/0000100885-94-000001.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100885/0000100885-94-000004.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100893/0000950144-94-000039.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100893/0000950144-94-000045.txt',\n",
       " 'https://www.sec.gov/Archives/edgar/data/100893/0000950144-94-000089.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Define the function sublist_per to make sublists inside a list for every \"step\" (=5000) elements\n",
    "#### so that once after every 5000 filing information have been extracted, they are saved to disk, and scraping can resume from where it was interrupted last time\n",
    "#### to avoid time loss due to system crash\n",
    "def sublist_per(source, step):\n",
    "    return [source[i-step:i] for i in list(range(len(source)))[::step][1:]] + [source[list(range(len(source)))[::step][-1]:]]\n",
    "\n",
    "#print(processed)\n",
    "web_url = sublist_per(web_url[:1045],100)\n",
    "web_url[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define a function count_occurrence to count the number of words in tup that pertaining to a list \n",
    "def count_occurrence(tup, lst): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst: \n",
    "            count+= 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "### Define a function count_negation to count cases where negation occurs within four or fewer words from a word identified in list.\n",
    "def count_negation(tup, lst, negation): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst:\n",
    "            if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "                neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "                for neighborw in neighbor:\n",
    "                    if neighborw in negation:\n",
    "                        count+= 1\n",
    "\n",
    "            if tup.index(item)-4 < 0:\n",
    "                pre = tup[0:tup.index(item)+4]\n",
    "                for prew in pre:\n",
    "                    if prew in negation:\n",
    "                        count+= 1\n",
    "                        \n",
    "            if tup.index(item)+4 > len(tup):\n",
    "                post = tup[tup.index(item)-4:len(tup)]\n",
    "                for postw in post:\n",
    "                    if postw in negation:\n",
    "                        count+= 1\n",
    "    return count\n",
    "\n",
    "############# Define a function to count the number of numerical occurrence in a string\n",
    "def number_digits(inputString):\n",
    "    return len(re.findall(r'\\s?\\d+\\W?\\d+\\s?', inputString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:04<00:00,  1.25s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:26<00:00,  2.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.27s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:22<00:00,  1.42s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.35s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:53<00:00,  1.74s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:30<00:00,  1.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:26<00:00,  2.06s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:05<00:00,  1.85s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [01:35<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "\n",
    "if os.path.exists(output_csv_dir) == False:\n",
    "    with open(output_csv_dir, mode='w') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        if (obj_type == '10-K') | (obj_type == '10-Q'):\n",
    "            writer.writerow(['web_url','nw_'+item_type, 'n_neg_lm_'+item_type, 'n_pos_lm_'+item_type,'n_neg_gi_'+item_type, 'n_pos_gi_'+item_type, \\\n",
    "                             'n_neg_hr_'+item_type, 'n_pos_hr_'+item_type, 'n_uctt_lm_'+item_type,'n_lit_lm_'+item_type, 'n_negation_lm_'+item_type, \\\n",
    "                             'n_negation_gi_'+item_type, 'n_negation_hr_'+item_type, 'fog_'+item_type, 'ari_'+item_type, 'smog_'+item_type])\n",
    "        else:\n",
    "            writer.writerow(['web_url','nw', 'n_neg_lm', 'n_pos_lm','n_neg_gi', 'n_pos_gi','n_neg_hr', 'n_pos_hr', 'n_uctt_lm','n_lit_lm', \\\n",
    "                             'n_negation_lm', 'n_negation_gi', 'n_negation_hr', 'fog', 'ari', 'smog'])\n",
    "for sublist in web_url:\n",
    "    ############ Full Text Raw Count\n",
    "    url_list = []\n",
    "\n",
    "    nw = []\n",
    "    #nvocab = []\n",
    "    #nsyllable = []\n",
    "    #nsentence = []\n",
    "\n",
    "    n_neg_lm = []\n",
    "    n_pos_lm = []\n",
    "    n_neg_gi = []\n",
    "    n_pos_gi = []\n",
    "    n_neg_hr = []\n",
    "    n_pos_hr = []\n",
    "    n_uctt_lm = []\n",
    "    n_lit_lm = []\n",
    "    #n_cstr_lm = []\n",
    "    #n_modal1_lm = []\n",
    "    #n_modal2_lm = []\n",
    "    #n_modal3_lm = []\n",
    "    n_negation_lm = []\n",
    "    n_negation_gi = []\n",
    "    n_negation_hr = []\n",
    "    \n",
    "    #fre = []\n",
    "    #fkg = []\n",
    "    #cl = []\n",
    "    fog = []\n",
    "    ari = []\n",
    "    smog = []\n",
    "\n",
    "    ############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "    for url in tqdm(sublist):\n",
    "        ### use url, instead of accnum, as key to link to id_data\n",
    "        url_list.append(url)\n",
    "        ############# Read txt file from EDGAR\n",
    "        time.sleep(time_waiting) # SEC does not allow to exceed 10 requests/sec\n",
    "        doc_resp = requests.get(url, headers=headers)\n",
    "        if doc_resp.status_code == 429:\n",
    "            time.sleep(10*60+5) # if exceeds cool off for 10 mins\n",
    "            doc_resp = requests.get(url, headers=headers)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        soup = BeautifulSoup(doc_resp.text, 'html.parser')\n",
    "        ############# Clean HTML tags and nondisplay section #####################\n",
    "        # 2.1).i): delete nondisplay section\n",
    "        nondisplay = soup.find('div', style=\"display:none;\") or soup.find('div', style=\"display:none\")\n",
    "        if nondisplay is not None:\n",
    "            _ = nondisplay.extract()\n",
    "\n",
    "        # 2.1).ii): delete tables that contains more than 4 numbers\n",
    "        table_tag = soup.find_all('table')\n",
    "        for tab in table_tag:\n",
    "            if number_digits(tab.get_text()) > 4:\n",
    "                _ = tab.extract()\n",
    "\n",
    "        # 2.3): delete html tags\n",
    "        contents = soup.get_text().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        ############# Clean HTML tags and nondisplay section #####################\n",
    "\n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens = word_tokenize(contents)\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "\n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        #vocab = sorted(set(words))\n",
    "\n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. nsyllable 4.nsentence 5. tone 6. readability\n",
    "\n",
    "        ## 1. nw\n",
    "        nw.append(len(words))\n",
    "\n",
    "        ## 2. nvocab\n",
    "        #nvocab.append(len(vocab))\n",
    "\n",
    "        ## 3. syllable\n",
    "        #nsyllable.append(textstat.syllable_count(contents))\n",
    "\n",
    "        ## 4. sentence\n",
    "        #nsentence.append(textstat.sentence_count(contents))\n",
    "\n",
    "        ## 5. tone\n",
    "        ### LM dictionary\n",
    "        n_neg_lm.append(count_occurrence(words, lm_neg))\n",
    "        n_pos_lm.append(count_occurrence(words, lm_pos))\n",
    "        n_uctt_lm.append(count_occurrence(words, lm_uctt))\n",
    "        n_lit_lm.append(count_occurrence(words, lm_lit))\n",
    "#         n_cstr_lm.append(count_occurrence(words, lm_cstr))\n",
    "#         n_modal1_lm.append(count_occurrence(words, lm_modal1))\n",
    "#         n_modal2_lm.append(count_occurrence(words, lm_modal2))\n",
    "#         n_modal3_lm.append(count_occurrence(words, lm_modal3))\n",
    "        n_negation_lm.append(count_negation(words, lm_pos, gt_negation))\n",
    "        ### General Inquirer dictionary\n",
    "        n_neg_gi.append(count_occurrence(words, gi_neg))\n",
    "        n_pos_gi.append(count_occurrence(words, gi_pos))\n",
    "        n_negation_gi.append(count_negation(words, gi_pos, gt_negation))\n",
    "        ### Henry dictionary\n",
    "        n_neg_hr.append(count_occurrence(words, hr_neg))\n",
    "        n_pos_hr.append(count_occurrence(words, hr_pos))\n",
    "        n_negation_hr.append(count_negation(words, gi_pos, gt_negation))\n",
    "\n",
    "        ## 4. readability\n",
    "#         fre_i = textstat.flesch_reading_ease(contents)\n",
    "#         if fre_i > 100:\n",
    "#             fre_i = 100\n",
    "#         if fre_i < 0:\n",
    "#             fre_i = float('NaN')\n",
    "#         fre.append(fre_i)\n",
    "\n",
    "#         fkg_i = textstat.flesch_kincaid_grade(contents)\n",
    "#         if fkg_i < 0:\n",
    "#             fkg_i = float('NaN')\n",
    "#         fkg.append(fkg_i)\n",
    "#         #RIX\n",
    "#         cl_i = textstat.coleman_liau_index(contents)\n",
    "#         if cl_i < 0:\n",
    "#             cl_i = float('NaN')\n",
    "#         cl.append(cl_i)\n",
    "\n",
    "        fog.append(textstat.gunning_fog(contents))\n",
    "        ari.append(textstat.automated_readability_index(contents))\n",
    "        smog.append(textstat.smog_index(contents))\n",
    "        #LIX\n",
    "    \n",
    "    #### save scraped data locally\n",
    "    if (obj_type == '10-K') | (obj_type == '10-Q'):\n",
    "        d = {'web_url': url_list, 'nw_'+item_type: nw, 'n_neg_lm_'+item_type: n_neg_lm, \\\n",
    "             'n_pos_lm_'+item_type: n_pos_lm, 'n_neg_gi_'+item_type: n_neg_gi, 'n_pos_gi_'+item_type: n_pos_gi, 'n_neg_hr_'+item_type: n_neg_hr, 'n_pos_hr_'+item_type: n_pos_hr, \\\n",
    "             'n_uctt_lm_'+item_type: n_uctt_lm, 'n_lit_lm_'+item_type: n_lit_lm, 'n_negation_lm_'+item_type: n_negation_lm, 'n_negation_gi_'+item_type: n_negation_gi, \\\n",
    "             'n_negation_hr_'+item_type: n_negation_hr, 'fog_'+item_type: fog, 'ari_'+item_type: ari, 'smog_'+item_type: smog}\n",
    "    else:\n",
    "        d = {'web_url': url_list, 'nw': nw, 'n_neg_lm': n_neg_lm, \\\n",
    "             'n_pos_lm': n_pos_lm, 'n_neg_gi': n_neg_gi, 'n_pos_gi': n_pos_gi, 'n_neg_hr': n_neg_hr, 'n_pos_hr': n_pos_hr, \\\n",
    "             'n_uctt_lm': n_uctt_lm, 'n_lit_lm': n_lit_lm, 'n_negation_lm': n_negation_lm, 'n_negation_gi': n_negation_gi, \\\n",
    "             'n_negation_hr': n_negation_hr, 'fog': fog, 'ari': ari, 'smog': smog}\n",
    "    \n",
    "    text_data = pd.DataFrame(data=d)\n",
    "    \n",
    "    text_data_saved = pd.read_csv(output_csv_dir)\n",
    "    text_data = pd.concat([text_data_saved, text_data])\n",
    "    text_data.to_csv(output_csv_dir, index=False)\n",
    "    # delete text_data_saved to release memory\n",
    "    del text_data_saved\n",
    "    del text_data\n",
    "    n = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ################### FOR SINGLE FILE INSPECTION ######################\n",
    "# #####################################################################\n",
    "\n",
    "# ############ Word Tokenization\n",
    "# ## Raw tokens: including punctuations, numbers etc.\n",
    "# with open(processed[1], 'r',  encoding = \"utf-8\") as file:\n",
    "#     contents = file.read().replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "# tokens = word_tokenize(contents)\n",
    "\n",
    "# #tokens\n",
    "\n",
    "# ## Convert all words into small cases\n",
    "# ## And keep tokens that purely consist of alphabetic characters only\n",
    "# words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# # words[2500:2600]\n",
    "# # vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrence(tup, lst): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst: \n",
    "#             count+= 1\n",
    "      \n",
    "#     return count\n",
    "\n",
    "# count_occurrence(words, lm_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing\n",
    "\n",
    "# def count_negation(tup, lst, negation): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst:\n",
    "#             if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "#                 neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "#                 for neighborw in neighbor:\n",
    "#                     if neighborw in negation:\n",
    "#                         count+= 1\n",
    "\n",
    "#             if tup.index(item)-4 < 0:\n",
    "#                 pre = tup[0:tup.index(item)+4]\n",
    "#                 for prew in pre:\n",
    "#                     if prew in negation:\n",
    "#                         count+= 1\n",
    "                        \n",
    "#             if tup.index(item)+4 > len(tup):\n",
    "#                 post = tup[tup.index(item)-4:len(tup)]\n",
    "#                 for postw in post:\n",
    "#                     if postw in negation:\n",
    "#                         count+= 1\n",
    "#     return count\n",
    "\n",
    "# count_negation(words, lm_pos, gt_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Winsorize words with lenth smaller than 1% and largr than 99% of the document\n",
    "# wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "# wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "# words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### See the most common 20 words\n",
    "# fdist = nltk.FreqDist(words)\n",
    "# fdist.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
